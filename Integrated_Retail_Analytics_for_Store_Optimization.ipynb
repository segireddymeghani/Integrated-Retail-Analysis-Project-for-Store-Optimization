{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vncDsAP0Gaoa"
      },
      "source": [
        "# **Project Name**    -\n",
        "\n",
        "Integrated Retail Analytics for Store Optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beRrZCGUAJYm"
      },
      "source": [
        "##### **Project Type**    - EDA\n",
        "##### **Contribution**    - Individual"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJNUwmbgGyua"
      },
      "source": [
        "# **Project Summary -**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6v_1wHtG2nS"
      },
      "source": [
        "This project focuses on analyzing multi-store retail data to provide insights and build predictive models for store optimization. The objective is to understand sales patterns, the impact of external factors, and strategies for improving business performance across different stores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6K7xa23Elo4"
      },
      "source": [
        "# **GitHub Link -**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1o69JH3Eqqn"
      },
      "source": [
        "Provide your GitHub Link here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQaldy8SH6Dl"
      },
      "source": [
        "# **Problem Statement**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpeJGUA3kjGy"
      },
      "source": [
        "How can integrated data from multiple sources (store, sales, and external features) be leveraged using analytics and machine learning to optimize store performance, improve sales forecasting, and support strategic retail decision-making"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDgbUHAGgjLW"
      },
      "source": [
        "# **General Guidelines** : -  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      },
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_i_v8NEhb9l"
      },
      "source": [
        "# ***Let's Begin !***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhfV-JJviCcP"
      },
      "source": [
        "## ***1. Know Your Data***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3lxredqlCYt"
      },
      "source": [
        "### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "outputs": [],
      "source": [
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "\n",
        "\n",
        "# Display settings\n",
        "pd.set_option(\"display.max_columns\", None)\n",
        "pd.set_option(\"display.max_rows\", 100)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RnN4peoiCZX"
      },
      "source": [
        "### Dataset Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "outputs": [],
      "source": [
        "# Load Dataset\n",
        "stores = pd.read_csv(\"stores data-set.csv\")\n",
        "sales = pd.read_csv(\"sales data-set.csv\")\n",
        "features = pd.read_csv(\"Features data set.csv\")\n",
        "\n",
        "print(\"Datasets Loaded Successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x71ZqKXriCWQ"
      },
      "source": [
        "### Dataset First View"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "outputs": [],
      "source": [
        "# Dataset First Look\n",
        "print(\"Stores Dataset:\")\n",
        "display(stores.head())\n",
        "\n",
        "print(\"\\nSales Dataset:\")\n",
        "display(sales.head())\n",
        "\n",
        "print(\"\\nFeatures Dataset:\")\n",
        "display(features.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hBIi_osiCS2"
      },
      "source": [
        "### Dataset Rows & Columns count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "outputs": [],
      "source": [
        "# Dataset Rows & Columns count\n",
        "print(\"Stores Dataset Shape:\", stores.shape)\n",
        "print(\"Sales Dataset Shape:\", sales.shape)\n",
        "print(\"Features Dataset Shape:\", features.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlHwYmJAmNHm"
      },
      "source": [
        "### Dataset Information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "outputs": [],
      "source": [
        "# Dataset Info\n",
        "print(\"Stores Dataset Info:\")\n",
        "print(stores.info())\n",
        "\n",
        "print(\"\\nSales Dataset Info:\")\n",
        "print(sales.info())\n",
        "\n",
        "print(\"\\nFeatures Dataset Info:\")\n",
        "print(features.info())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35m5QtbWiB9F"
      },
      "source": [
        "#### Duplicate Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "outputs": [],
      "source": [
        "# Dataset Duplicate Value Count\n",
        "print(\"Duplicate Rows in Stores:\", stores.duplicated().sum())\n",
        "print(\"Duplicate Rows in Sales:\", sales.duplicated().sum())\n",
        "print(\"Duplicate Rows in Features:\", features.duplicated().sum())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoPl-ycgm1ru"
      },
      "source": [
        "#### Missing Values/Null Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "outputs": [],
      "source": [
        "# Missing Values/Null Values Count\n",
        "print(\"Missing Values in Stores:\\n\", stores.isnull().sum())\n",
        "print(\"\\nMissing Values in Sales:\\n\", sales.isnull().sum())\n",
        "print(\"\\nMissing Values in Features:\\n\", features.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "outputs": [],
      "source": [
        "# Visualizing the missing values\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.heatmap(features.isnull(), cbar=False, cmap=\"viridis\")\n",
        "plt.title(\"Missing Values in Features Dataset\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0kj-8xxnORC"
      },
      "source": [
        "### What did you know about your dataset?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfoNAAC-nUe_"
      },
      "source": [
        "Answer :\n",
        "The dataset consists of three files:\n",
        "Stores dataset: Contains information about different store types and sizes.\n",
        "Sales dataset: Weekly sales data for each store and department.\n",
        "Features dataset: Additional information such as temperature, fuel price, CPI, and unemployment rate for each store and week."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      },
      "source": [
        "## ***2. Understanding Your Variables***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "outputs": [],
      "source": [
        "# Dataset Columns\n",
        "print(\"Stores Columns:\", stores.columns.tolist())\n",
        "print(\"Sales Columns:\", sales.columns.tolist())\n",
        "print(\"Features Columns:\", features.columns.tolist())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "outputs": [],
      "source": [
        "# Dataset Describe\n",
        "print(\"Stores Dataset Description:\")\n",
        "display(stores.describe())\n",
        "\n",
        "print(\"\\nSales Dataset Description:\")\n",
        "display(sales.describe())\n",
        "\n",
        "print(\"\\nFeatures Dataset Description:\")\n",
        "display(features.describe())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBTbrJXOngz2"
      },
      "source": [
        "### Variables Description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJV4KIxSnxay"
      },
      "source": [
        "Answer :\n",
        "Store: Store ID\n",
        "\n",
        "Dept: Department number\n",
        "\n",
        "Date: Week of sales\n",
        "\n",
        "Weekly_Sales: Total sales for the department in that week\n",
        "\n",
        "IsHoliday: Whether the week was a holiday week (True/False)\n",
        "\n",
        "Size: Store size in square feet\n",
        "\n",
        "Type: Store format/type (A, B, C)\n",
        "\n",
        "Temperature: Temperature in the region\n",
        "\n",
        "Fuel_Price: Cost of fuel in the region\n",
        "\n",
        "CPI: Consumer Price Index\n",
        "\n",
        "Unemployment: Unemployment rate\n",
        "\n",
        "MarkDown1-5: Promotional markdowns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3PMJOP6ngxN"
      },
      "source": [
        "### Check Unique Values for each variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "outputs": [],
      "source": [
        "# Check Unique Values for each variable.\n",
        "print(\"Stores Dataset Unique Values:\\n\", stores.nunique())\n",
        "print(\"\\nSales Dataset Unique Values:\\n\", sales.nunique())\n",
        "print(\"\\nFeatures Dataset Unique Values:\\n\", features.nunique())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dauF4eBmngu3"
      },
      "source": [
        "## 3. ***Data Wrangling***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKJF3rekwFvQ"
      },
      "source": [
        "### Data Wrangling Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "outputs": [],
      "source": [
        "# Merge all three datasets\n",
        "data = sales.merge(stores, on=\"Store\").merge(features, on=[\"Store\", \"Date\"])\n",
        "\n",
        "# Handling Missing Values\n",
        "data.fillna(0, inplace=True)   # replacing nulls with 0 (for MarkDowns, CPI, Unemployment)\n",
        "\n",
        "# Convert Date column to datetime (handling dd/mm/yyyy format)\n",
        "data['Date'] = pd.to_datetime(data['Date'], dayfirst=True, errors='coerce')\n",
        "\n",
        "# Final dataset ready\n",
        "print(\"Final Dataset Shape:\", data.shape)\n",
        "display(data.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSa1f5Uengrz"
      },
      "source": [
        "### What all manipulations have you done and insights you found?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbyXE7I1olp8"
      },
      "source": [
        "Answer :\n",
        "Merged the three datasets into a single unified dataset using Store, Dept, and Date.\n",
        "Handled missing values by replacing them with 0 (suitable for MarkDowns) or interpolating if required.\n",
        "Converted Date column to datetime format for time-series analysis.\n",
        "Found that sales vary heavily during holiday weeks and depend on store type and size.\n",
        "Features like fuel price, CPI, and unemployment show potential correlation with weekly sales."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GF8Ens_Soomf"
      },
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wOQAZs5pc--"
      },
      "source": [
        "#### Chart - 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "outputs": [],
      "source": [
        "# Chart - 1 visualization code\n",
        "import matplotlib.pyplot as plt\n",
        "# Aggregate sales by Date\n",
        "sales_trend = data.groupby(\"Date\")[\"Weekly_Sales\"].sum().reset_index()\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(sales_trend[\"Date\"], sales_trend[\"Weekly_Sales\"], color=\"blue\", linewidth=2)\n",
        "plt.title(\"Weekly Sales Trend Over Time\", fontsize=16)\n",
        "plt.xlabel(\"Date\", fontsize=12)\n",
        "plt.ylabel(\"Total Weekly Sales\", fontsize=12)\n",
        "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5QZ13OEpz2H"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XESiWehPqBRc"
      },
      "source": [
        "Answer :\n",
        "A line chart is best suited for time-series data because it clearly shows trends, seasonality, and fluctuations in weekly sales.\n",
        "It helps identify holiday effects, promotional spikes, and slumps over time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_j1G7yiqdRP"
      },
      "source": [
        "Answer :\n",
        "Sales show a clear seasonal trend, with noticeable spikes during festive/holiday seasons (e.g., Thanksgiving, Christmas).\n",
        "There are periods where sales dip significantly, possibly due to off-seasons or ineffective promotions.\n",
        "Consistency of sales over time varies — some weeks are highly profitable, others underperform."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "448CDAPjqfQr"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cspy4FjqxJW"
      },
      "source": [
        "Answer :\n",
        "Positive Business Impact:\n",
        "Identifying peak sales weeks allows better inventory planning, staff allocation, and targeted marketing campaigns.\n",
        "Retailers can prepare in advance for high-demand seasons to maximize revenue.\n",
        "\n",
        "Negative Growth Insight:\n",
        "Weeks with sharp sales declines indicate either poor promotions, economic downturn, or stockouts.\n",
        "If not addressed, these dips may cause customer dissatisfaction and loss to competitors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSlN3yHqYklG"
      },
      "source": [
        "#### Chart - 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "outputs": [],
      "source": [
        "# Chart - 2 visualization code\n",
        "data[\"Month\"] = data[\"Date\"].dt.month\n",
        "monthly_sales = data.groupby(\"Month\")[\"Weekly_Sales\"].sum().reset_index()\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.barplot(x=\"Month\", y=\"Weekly_Sales\", data=monthly_sales, palette=\"viridis\")\n",
        "plt.title(\"Monthly Sales Trend\", fontsize=16)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6dVpIINYklI"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aaW0BYyYklI"
      },
      "source": [
        "Answer : Bar plot reveals monthly seasonality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijmpgYnKYklI"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSx9atu2YklI"
      },
      "source": [
        "Answer : Nov–Dec highest sales (holiday season)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JiQyfWJYklI"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcBbebzrYklV"
      },
      "source": [
        "Answer : Helps optimize holiday promotions & supply chain."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EM7whBJCYoAo"
      },
      "source": [
        "#### Chart - 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "outputs": [],
      "source": [
        "# Chart - 3 visualization code\n",
        "store_sales = data.groupby(\"Store\")[\"Weekly_Sales\"].mean().reset_index()\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.barplot(x=\"Store\", y=\"Weekly_Sales\", data=store_sales, palette=\"coolwarm\")\n",
        "plt.title(\"Average Weekly Sales per Store\", fontsize=16)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fge-S5ZAYoAp"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dBItgRVYoAp"
      },
      "source": [
        "Answer : Compare performance across stores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85gYPyotYoAp"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jstXR6OYoAp"
      },
      "source": [
        "Answer : Some stores perform significantly better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoGjAbkUYoAp"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      },
      "source": [
        "Answer : Replicate best practices from high-performing stores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Of9eVA-YrdM"
      },
      "source": [
        "#### Chart - 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "outputs": [],
      "source": [
        "# Chart - 4 visualization code\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.boxplot(x=\"IsHoliday_x\", y=\"Weekly_Sales\", data=data, palette=\"Set2\")\n",
        "plt.title(\"Sales Distribution: Holiday vs Non-Holiday Weeks\", fontsize=16)\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iky9q4vBYrdO"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJRCwT6DYrdO"
      },
      "source": [
        "Answer : Boxplot highlights difference in sales distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6T5p64dYrdO"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      },
      "source": [
        "Answer : Holiday weeks drive much higher sales."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-Ehk30pYrdP"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLNxxz7MYrdP"
      },
      "source": [
        "Answer : Focus promotions around holidays."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bamQiAODYuh1"
      },
      "source": [
        "#### Chart - 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "outputs": [],
      "source": [
        "# Chart - 5 visualization code\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.barplot(x=\"Type\", y=\"Weekly_Sales\", data=data, estimator=np.mean, palette=\"pastel\")\n",
        "plt.title(\"Average Sales by Store Type\", fontsize=16)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcxuIMRPYuh3"
      },
      "source": [
        "Answer : Compare store types (A, B, C)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwzvFGzlYuh3"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyqkiB8YYuh3"
      },
      "source": [
        "Answer : Type A stores outperform consistently."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYpmQ266Yuh3"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      },
      "source": [
        "Answer : Expansion strategy → build more Type A stores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OH-pJp9IphqM"
      },
      "source": [
        "#### Chart - 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "outputs": [],
      "source": [
        "# Chart - 6 visualization code\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.scatterplot(x=\"Size\", y=\"Weekly_Sales\", hue=\"Type\", data=data, alpha=0.6)\n",
        "plt.title(\"Store Size vs Weekly Sales\", fontsize=16)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbFf2-_FphqN"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loh7H2nzphqN"
      },
      "source": [
        "Answer : Scatterplot shows correlation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ouA3fa0phqN"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VECbqPI7phqN"
      },
      "source": [
        "Answer : Larger stores generate higher sales."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Seke61FWphqN"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DW4_bGpfphqN"
      },
      "source": [
        "Answer : Allocate more space for bigger stores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIIx-8_IphqN"
      },
      "source": [
        "#### Chart - 7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "outputs": [],
      "source": [
        "# Chart - 7 visualization code\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.scatterplot(x=\"Fuel_Price\", y=\"Weekly_Sales\", data=data, alpha=0.5)\n",
        "plt.title(\"Fuel Price vs Weekly Sales\", fontsize=16)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t27r6nlMphqO"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iv6ro40sphqO"
      },
      "source": [
        "Answer : See external factor influence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2jJGEOYphqO"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Po6ZPi4hphqO"
      },
      "source": [
        "Answer : High fuel prices slightly reduce sales."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0JNsNcRphqO"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvSq8iUTphqO"
      },
      "source": [
        "Answer : Plan discounts when fuel prices are high."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZR9WyysphqO"
      },
      "source": [
        "#### Chart - 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "outputs": [],
      "source": [
        "# Chart - 8 visualization code\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.scatterplot(x=\"CPI\", y=\"Weekly_Sales\", data=data, alpha=0.5, color=\"red\")\n",
        "plt.title(\"CPI vs Weekly Sales\", fontsize=16)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jj7wYXLtphqO"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ob8u6rCTphqO"
      },
      "source": [
        "Answer : CPI indicates inflation impact."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZrbJ2SmphqO"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZtgC_hjphqO"
      },
      "source": [
        "Answer : Higher CPI correlates with lower sales."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFu4xreNphqO"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ey_0qi68phqO"
      },
      "source": [
        "Answer :\n",
        "Positive Impact: Understands customer sensitivity to fuel prices, useful for pricing and discount strategies.\n",
        "Negative Growth: If sales drop heavily with rising fuel prices, long-term risk exists."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJ55k-q6phqO"
      },
      "source": [
        "#### Chart - 9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "outputs": [],
      "source": [
        "# Chart - 9 visualization code\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.scatterplot(x=\"Unemployment\", y=\"Weekly_Sales\", data=data, alpha=0.5, color=\"green\")\n",
        "plt.title(\"Unemployment vs Weekly Sales\", fontsize=16)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCFgpxoyphqP"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVxDimi2phqP"
      },
      "source": [
        "Answer : Measures macroeconomic impact."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVtJsKN_phqQ"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngGi97qjphqQ"
      },
      "source": [
        "Answer : Higher unemployment leads to reduced sales."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lssrdh5qphqQ"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBpY5ekJphqQ"
      },
      "source": [
        "Answer :\n",
        "Positive Impact: Reveals inflation impact on customer spending. Useful for pricing strategies.\n",
        "Negative Growth: High CPI reducing sales indicates lower affordability among customers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      },
      "source": [
        "#### Chart - 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "outputs": [],
      "source": [
        "# Chart - 10 visualization code\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.histplot(data[\"Weekly_Sales\"], bins=50, kde=True, color=\"purple\")\n",
        "plt.title(\"Distribution of Weekly Sales\", fontsize=16)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1M8mcRywphqQ"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8agQvks0phqQ"
      },
      "source": [
        "Answer : Distribution check."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgIPom80phqQ"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qp13pnNzphqQ"
      },
      "source": [
        "Answer : Most weeks have moderate sales, few weeks extremely high."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMzcOPDDphqR"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4Ka1PC2phqR"
      },
      "source": [
        "Answer :\n",
        "Positive Impact: Links macroeconomic conditions with sales, helping in risk planning.\n",
        "Negative Growth: High unemployment correlating with sales decline signals potential business vulnerability."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-EpHcCOp1ci"
      },
      "source": [
        "#### Chart - 11"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "outputs": [],
      "source": [
        "# Chart - 11 visualization code\n",
        "plt.figure(figsize=(14,6))\n",
        "sns.boxplot(x=\"Store\", y=\"Weekly_Sales\", data=data, palette=\"Set3\")\n",
        "plt.title(\"Sales Distribution Across Stores\", fontsize=16)\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_VqEhTip1ck"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vsMzt_np1ck"
      },
      "source": [
        "Answer : Identify outlier stores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zGJKyg5p1ck"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      },
      "source": [
        "Answer : Some stores consistently outperform; others lag."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "druuKYZpp1ck"
      },
      "source": [
        "Answer :\n",
        "Yes, the boxplot can also reveal negative insights:\n",
        "Stores with very low median sales or consistent low performance → may indicate poor location, low footfall, or ineffective store management.\n",
        "Stores with high variability and frequent low sales weeks → indicate risk of revenue instability, which may affect profit margins.\n",
        "Presence of outliers with very low sales could suggest stock-outs, operational issues, or lack of promotions.\n",
        "Negative Growth Justification: These underperforming stores, if not addressed, can drag down overall revenue and increase operational costs without proportional returns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3dbpmDWp1ck"
      },
      "source": [
        "#### Chart - 12"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "outputs": [],
      "source": [
        "# Chart - 12 visualization code\n",
        "holiday_sales = data[data[\"IsHoliday_x\"]==True].groupby(\"Year\")[\"Weekly_Sales\"].sum().reset_index()\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.barplot(x=\"Year\", y=\"Weekly_Sales\", data=holiday_sales, palette=\"magma\")\n",
        "plt.title(\"Holiday Sales per Year\")\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylSl6qgtp1ck"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2xqNkiQp1ck"
      },
      "source": [
        "Answer : Track holiday sales growth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWILFDl5p1ck"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-lUsV2mp1ck"
      },
      "source": [
        "Answer : Sales increase year by year during holidays."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7G43BXep1ck"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wwDJXsLp1cl"
      },
      "source": [
        "Answer :\n",
        "Positive Impact: Shows sales variation within store types, guiding investment strategy.\n",
        "Negative Growth: Wide variation in same store type suggests inconsistent operations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ag9LCva-p1cl"
      },
      "source": [
        "#### Chart - 13"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "outputs": [],
      "source": [
        "# Chart - 13 visualization code\n",
        "markdown_cols = [\"MarkDown1\",\"MarkDown2\",\"MarkDown3\",\"MarkDown4\",\"MarkDown5\"]\n",
        "\n",
        "for col in markdown_cols:\n",
        "    plt.figure(figsize=(8,5))\n",
        "    sns.scatterplot(x=data[col], y=data[\"Weekly_Sales\"], alpha=0.5)\n",
        "    plt.title(f\"{col} vs Weekly Sales\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6MkPsBcp1cl"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V22bRsFWp1cl"
      },
      "source": [
        "Answer : Analyze discount campaigns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cELzS2fp1cl"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      },
      "source": [
        "Answer : Markdown1 and Markdown2 drive sales more effectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MPXvC8up1cl"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GL8l1tdLp1cl"
      },
      "source": [
        "Answer :\n",
        "Positive Impact: Identifies normal sales ranges, helping in planning promotions.\n",
        "Negative Growth: If too many weeks fall below expected average, indicates revenue risks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NC_X3p0fY2L0"
      },
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "outputs": [],
      "source": [
        "# Correlation Heatmap visualization code\n",
        "plt.figure(figsize=(12,8))\n",
        "\n",
        "# Select only numeric columns\n",
        "numeric_data = data.select_dtypes(include=['float64', 'int64'])\n",
        "\n",
        "sns.heatmap(numeric_data.corr(), annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
        "plt.title(\"Correlation Heatmap\", fontsize=16)\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      },
      "source": [
        "Answer : Correlation check between variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfSqtnDqZNRR"
      },
      "source": [
        "Answer : CPI, Unemployment negatively correlated with sales."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q29F0dvdveiT"
      },
      "source": [
        "#### Chart - 15 - Pair Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "outputs": [],
      "source": [
        "# Pair Plot visualization code\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Selecting relevant numerical columns for pair plot\n",
        "numeric_cols = ['Weekly_Sales', 'Size', 'Temperature', 'Fuel_Price', 'CPI', 'Unemployment']\n",
        "\n",
        "# Pair plot visualization\n",
        "plt.figure(figsize=(12,10))\n",
        "sns.pairplot(data[numeric_cols], diag_kind='kde', corner=True, plot_kws={'alpha':0.5})\n",
        "plt.suptitle(\"Pair Plot of Key Numerical Features\", fontsize=16, y=1.02)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXh0U9oCveiU"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMmPjTByveiU"
      },
      "source": [
        "Answer :\n",
        "A pair plot is useful to visualize relationships between multiple numerical variables at once.\n",
        "It helps in identifying correlations, trends, and patterns between features like Weekly_Sales, Size, Temperature, CPI, etc.\n",
        "It also shows distributions of individual variables along the diagonal, making it easier to spot outliers or skewed data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22aHeOlLveiV"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPQ8RGwHveiV"
      },
      "source": [
        "Answer :\n",
        "Positive correlation between Store Size and Weekly Sales → larger stores tend to generate more sales.\n",
        "Weak correlation between Temperature and Weekly Sales → weather might not strongly influence sales across all stores.\n",
        "Certain features (like CPI, Unemployment, Fuel_Price) show little visible trend with sales → might not be strong predictors individually."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-ATYxFrGrvw"
      },
      "source": [
        "## ***5. Hypothesis Testing***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      },
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7MS06SUHkB-"
      },
      "source": [
        "Answer :\n",
        "Hypothetical Statement 1 – Holiday Effect on Sales\n",
        "\n",
        "Null Hypothesis (H₀): Weekly sales during holiday weeks are not significantly different from non-holiday weeks.\n",
        "Alternate Hypothesis (H₁): Weekly sales during holiday weeks are significantly different from non-holiday weeks.\n",
        "Test Used: Independent Samples t-test (Holiday vs Non-Holiday sales).\n",
        "Reason: Comparing mean sales across two independent groups.\n",
        "\n",
        "Hypothetical Statement 2 – Store Type and Sales Performance\n",
        "\n",
        "Null Hypothesis (H₀): The mean weekly sales are the same across all store types (A, B, C).\n",
        "Alternate Hypothesis (H₁): At least one store type has significantly different mean weekly sales.\n",
        "Test Used: One-Way ANOVA.\n",
        "Reason: We are comparing mean sales across more than two groups (Store types).\n",
        "\n",
        "Hypothetical Statement 3 – Unemployment Rate Impact on Sales\n",
        "\n",
        "Null Hypothesis (H₀): There is no correlation between unemployment rate and weekly sales.\n",
        "Alternate Hypothesis (H₁): There is a significant correlation between unemployment rate and weekly sales.\n",
        "Test Used: Pearson Correlation Test.\n",
        "Reason: Both unemployment rate and weekly sales are continuous variables; correlation test is suitable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yEUt7NnHlrM"
      },
      "source": [
        "### Hypothetical Statement - 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      },
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HI9ZP0laH0D-"
      },
      "source": [
        "Answer :\n",
        "Null Hypothesis (H₀): Weekly sales during holiday weeks are not significantly different from weekly sales during non-holiday weeks.\n",
        "Alternate Hypothesis (H₁): Weekly sales during holiday weeks are significantly different from weekly sales during non-holiday weeks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I79__PHVH19G"
      },
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "outputs": [],
      "source": [
        "data = sales.merge(stores, on=\"Store\").merge(features, on=[\"Store\", \"Date\"])\n",
        "\n",
        "# Handle missing values\n",
        "data.fillna(0, inplace=True)\n",
        "\n",
        "# Convert Date column\n",
        "data['Date'] = pd.to_datetime(data['Date'], dayfirst=True, errors=\"coerce\")\n",
        "\n",
        "# ✅ Check if dataset is ready\n",
        "print(\"Dataset Shape:\", data.shape)\n",
        "print(\"Columns:\", data.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ou-I18pAyIpj"
      },
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2U0kk00ygSB"
      },
      "source": [
        "Answer : I have performed an Independent Two-Sample t-test (Student’s t-test) to obtain the p-value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fF3858GYyt-u"
      },
      "source": [
        "##### Why did you choose the specific statistical test?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HO4K0gP5y3B4"
      },
      "source": [
        "Answer :\n",
        "I chose the t-test because:\n",
        "We are comparing the mean weekly sales between two independent groups:\n",
        "Holiday weeks (IsHoliday = True)\n",
        "Non-holiday weeks (IsHoliday = False)\n",
        "The t-test is suitable when we want to check if the difference in means between two groups is statistically significant.\n",
        "Our dependent variable (Weekly_Sales) is continuous, and the independent variable (IsHoliday) is categorical with two levels, making the t-test the correct choice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_0_7-oCpUZd"
      },
      "source": [
        "### Hypothetical Statement - 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwyV_J3ipUZe"
      },
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      },
      "source": [
        "Answer :\n",
        "Null Hypothesis (H₀):\n",
        "The average weekly sales across different store types (A, B, C) are the same.\n",
        "Alternate Hypothesis (H₁):\n",
        "At least one store type has a different average weekly sales compared to the others."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yB-zSqbpUZe"
      },
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "outputs": [],
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from scipy.stats import f_oneway\n",
        "\n",
        "# Split weekly sales by store type\n",
        "type_a_sales = data[data[\"Type\"] == \"A\"][\"Weekly_Sales\"]\n",
        "type_b_sales = data[data[\"Type\"] == \"B\"][\"Weekly_Sales\"]\n",
        "type_c_sales = data[data[\"Type\"] == \"C\"][\"Weekly_Sales\"]\n",
        "\n",
        "# Perform One-Way ANOVA test\n",
        "f_stat, p_value = f_oneway(type_a_sales, type_b_sales, type_c_sales)\n",
        "\n",
        "print(\"ANOVA F-statistic:\", f_stat)\n",
        "print(\"P-value:\", p_value)\n",
        "\n",
        "# Interpretation\n",
        "if p_value < 0.05:\n",
        "    print(\"Reject Null Hypothesis → At least one store type has significantly different sales.\")\n",
        "else:\n",
        "    print(\"Fail to Reject Null Hypothesis → No significant difference in sales across store types.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUvejAfpUZe"
      },
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLDrPz7HpUZf"
      },
      "source": [
        "Answer : I used the One-Way ANOVA (Analysis of Variance) test."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fd15vwWVpUZf"
      },
      "source": [
        "##### Why did you choose the specific statistical test?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xOGYyiBpUZf"
      },
      "source": [
        "Answer :\n",
        "Because we are comparing the average weekly sales across more than two groups (Store Types A, B, and C).\n",
        "A t-test can only compare two groups at a time, whereas ANOVA is specifically designed to test whether there is a statistically significant difference in the means of three or more groups simultaneously."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bn_IUdTipZyH"
      },
      "source": [
        "### Hypothetical Statement - 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49K5P_iCpZyH"
      },
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gWI5rT9pZyH"
      },
      "source": [
        "Answer :\n",
        "Null Hypothesis (H₀):\n",
        "Weekly sales are not significantly affected by promotions/markdown discounts.\n",
        "Alternate Hypothesis (H₁):\n",
        "Weekly sales are significantly affected by promotions/markdown discounts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nff-vKELpZyI"
      },
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "outputs": [],
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# Combine all markdown columns into one feature (since multiple markdown columns exist)\n",
        "data[\"Total_MarkDown\"] = (\n",
        "    data[\"MarkDown1\"] + data[\"MarkDown2\"] + data[\"MarkDown3\"] + data[\"MarkDown4\"] + data[\"MarkDown5\"]\n",
        ")\n",
        "\n",
        "# Perform Pearson Correlation Test between Weekly_Sales and Total_MarkDown\n",
        "corr, p_value = pearsonr(data[\"Weekly_Sales\"], data[\"Total_MarkDown\"])\n",
        "\n",
        "print(\"Correlation Coefficient:\", corr)\n",
        "print(\"P-Value:\", p_value)\n",
        "\n",
        "# Interpretation\n",
        "if p_value < 0.05:\n",
        "    print(\"Reject Null Hypothesis → Promotions/Markdowns significantly affect Weekly Sales.\")\n",
        "else:\n",
        "    print(\"Fail to Reject Null Hypothesis → No significant effect of Promotions/Markdowns on Weekly Sales.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLW572S8pZyI"
      },
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytWJ8v15pZyI"
      },
      "source": [
        "Answer : I used the Pearson Correlation Test."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWbDXHzopZyI"
      },
      "source": [
        "##### Why did you choose the specific statistical test?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M99G98V6pZyI"
      },
      "source": [
        "Answer :\n",
        "because both Weekly Sales and Promotions/Markdown values are continuous numeric variables. Pearson correlation helps measure the strength and direction of the linear relationship between these two variables while also providing a p-value to test statistical significance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLjJCtPM0KBk"
      },
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiyOF9F70UgQ"
      },
      "source": [
        "### 1. Handling Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "outputs": [],
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "# -----------------------------\n",
        "# Check missing values\n",
        "print(\"Missing values before imputation:\\n\", data.isnull().sum())\n",
        "\n",
        "# Fill missing numerical values (e.g., MarkDowns, CPI, Unemployment) with 0\n",
        "num_cols = [\"MarkDown1\", \"MarkDown2\", \"MarkDown3\", \"MarkDown4\", \"MarkDown5\", \"CPI\", \"Unemployment\"]\n",
        "data[num_cols] = data[num_cols].fillna(0)\n",
        "\n",
        "# Fill missing categorical values (if any) with mode\n",
        "cat_cols = data.select_dtypes(include=\"object\").columns\n",
        "for col in cat_cols:\n",
        "    data[col] = data[col].fillna(data[col].mode()[0])\n",
        "\n",
        "# Verify missing values handled\n",
        "print(\"\\nMissing values after imputation:\\n\", data.isnull().sum())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wuGOrhz0itI"
      },
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ixusLtI0pqI"
      },
      "source": [
        "Answer :\n",
        "In this dataset, no missing values were found. However, to make the dataset analysis-ready and robust for future updates, I applied the following imputation strategies:\n",
        "\n",
        "Numerical Columns (e.g., MarkDowns, CPI, Unemployment):\n",
        "\n",
        "Missing values were replaced with 0.\n",
        "\n",
        "Rationale: For features like markdowns, a missing value usually means no discount was applied, and for CPI/Unemployment, replacing with 0 ensures no bias is introduced while avoiding row deletion.\n",
        "\n",
        "Categorical Columns (e.g., Store Type, Holiday Flag):\n",
        "\n",
        "Missing values were filled using the mode (most frequent value).\n",
        "\n",
        "Rationale: Mode imputation preserves the most common category, ensuring consistency in categorical data without creating artificial categories."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "id1riN9m0vUs"
      },
      "source": [
        "### 2. Handling Outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "outputs": [],
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "# -----------------------------\n",
        "import numpy as np\n",
        "\n",
        "# List of numerical columns where outliers are likely\n",
        "num_cols = [\"Weekly_Sales\", \"Fuel_Price\", \"CPI\", \"Unemployment\", \"Temperature\",\n",
        "            \"MarkDown1\", \"MarkDown2\", \"MarkDown3\", \"MarkDown4\", \"MarkDown5\"]\n",
        "\n",
        "def remove_outliers_iqr(df, col):\n",
        "    Q1 = df[col].quantile(0.25)\n",
        "    Q3 = df[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "    # Clip values instead of removing rows\n",
        "    df[col] = np.where(df[col] < lower_bound, lower_bound,\n",
        "                       np.where(df[col] > upper_bound, upper_bound, df[col]))\n",
        "    return df\n",
        "\n",
        "# Apply IQR clipping to all numeric columns\n",
        "for col in num_cols:\n",
        "    data = remove_outliers_iqr(data, col)\n",
        "\n",
        "print(\"✅ Outlier treatment done using IQR method with clipping.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "578E2V7j08f6"
      },
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGZz5OrT1HH-"
      },
      "source": [
        "Answer :\n",
        "I used the Interquartile Range (IQR) Method for handling outliers in numerical columns such as Weekly_Sales, Fuel_Price, CPI, Unemployment, Temperature, and MarkDowns.\n",
        "\n",
        "Outliers were detected using the standard rule:\n",
        "\n",
        "Lower Bound = Q1 – 1.5 × IQR\n",
        "\n",
        "Upper Bound = Q3 + 1.5 × IQR\n",
        "\n",
        "Instead of dropping the outlier rows (which would reduce dataset size), I applied clipping:\n",
        "\n",
        "Values below the lower bound were capped at the lower bound.\n",
        "\n",
        "Values above the upper bound were capped at the upper bound."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89xtkJwZ18nB"
      },
      "source": [
        "### 3. Categorical Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "outputs": [],
      "source": [
        "# Encode your categorical columns\n",
        "# -----------------------------\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Identify categorical columns\n",
        "cat_cols = data.select_dtypes(include=\"object\").columns\n",
        "print(\"Categorical Columns:\", list(cat_cols))\n",
        "\n",
        "# Apply Label Encoding for categorical columns\n",
        "le = LabelEncoder()\n",
        "for col in cat_cols:\n",
        "    data[col] = le.fit_transform(data[col])\n",
        "\n",
        "print(\"✅ Categorical columns encoded successfully.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67NQN5KX2AMe"
      },
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDaue5h32n_G"
      },
      "source": [
        "Answer :\n",
        "I used Label Encoding for categorical features such as Type and other string-based variables in the dataset.\n",
        "\n",
        "Reason for using Label Encoding:\n",
        "\n",
        "Converts categories into numeric codes (e.g., A, B, C → 0, 1, 2).\n",
        "\n",
        "Simple and efficient for tree-based models like Random Forest and XGBoost, which can handle integer-based categories directly.\n",
        "\n",
        "Helps avoid unnecessary expansion of the dataset, unlike One-Hot Encoding which increases dimensionality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iwf50b-R2tYG"
      },
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMQiZwjn3iu7"
      },
      "source": [
        "#### 1. Expand Contraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "outputs": [],
      "source": [
        "# Expand Contraction\n",
        "# -----------------------------\n",
        "import re\n",
        "\n",
        "# Dictionary of common contractions\n",
        "contractions_dict = {\n",
        "    \"can't\": \"cannot\",\n",
        "    \"won't\": \"will not\",\n",
        "    \"n't\": \" not\",\n",
        "    \"'re\": \" are\",\n",
        "    \"'s\": \" is\",\n",
        "    \"'d\": \" would\",\n",
        "    \"'ll\": \" will\",\n",
        "    \"'t\": \" not\",\n",
        "    \"'ve\": \" have\",\n",
        "    \"'m\": \" am\"\n",
        "}\n",
        "\n",
        "def expand_contractions(text):\n",
        "    pattern = re.compile('({})'.format('|'.join(contractions_dict.keys())),\n",
        "                         flags=re.IGNORECASE|re.DOTALL)\n",
        "    def replace(match):\n",
        "        return contractions_dict[match.group(0).lower()]\n",
        "    return pattern.sub(replace, text)\n",
        "\n",
        "# Example text\n",
        "sample_text = \"I can't go to the store because it's closed.\"\n",
        "expanded_text = expand_contractions(sample_text)\n",
        "\n",
        "print(\"Before:\", sample_text)\n",
        "print(\"After:\", expanded_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVIkgGqN3qsr"
      },
      "source": [
        "#### 2. Lower Casing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "outputs": [],
      "source": [
        "# Lower Casing\n",
        "def to_lower(text):\n",
        "    return text.lower()\n",
        "\n",
        "sample_text = \"This is My TEXT with MIXED Case.\"\n",
        "print(\"Before:\", sample_text)\n",
        "print(\"After:\", to_lower(sample_text))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkPnILGE3zoT"
      },
      "source": [
        "#### 3. Removing Punctuations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "outputs": [],
      "source": [
        "# Remove Punctuations\n",
        "import string\n",
        "\n",
        "def remove_punctuations(text):\n",
        "    return text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "sample_text = \"Hello!!! This, right here; is a test...\"\n",
        "print(\"Before:\", sample_text)\n",
        "print(\"After:\", remove_punctuations(sample_text))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hlsf0x5436Go"
      },
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "outputs": [],
      "source": [
        "# Remove URLs & Remove words and digits contain digits\n",
        "import re\n",
        "\n",
        "def remove_urls_digits(text):\n",
        "    # Remove URLs\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n",
        "    # Remove words with digits\n",
        "    text = re.sub(r'\\w*\\d\\w*', '', text)\n",
        "    return text\n",
        "\n",
        "sample_text = \"Check out https://example.com for 2good offers!!\"\n",
        "print(\"Before:\", sample_text)\n",
        "print(\"After:\", remove_urls_digits(sample_text))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mT9DMSJo4nBL"
      },
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "outputs": [],
      "source": [
        "# Remove Stopwords\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    words = text.split()\n",
        "    return \" \".join([word for word in words if word.lower() not in stop_words])\n",
        "\n",
        "sample_text = \"This is an example showing how stopwords are removed\"\n",
        "print(\"Before:\", sample_text)\n",
        "print(\"After:\", remove_stopwords(sample_text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "outputs": [],
      "source": [
        "# Remove White spaces\n",
        "def remove_whitespace(text):\n",
        "    return \" \".join(text.split())\n",
        "\n",
        "sample_text2 = \"This    sentence   has   extra   spaces. \"\n",
        "print(\"Before:\", repr(sample_text2))\n",
        "print(\"After:\", repr(remove_whitespace(sample_text2)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c49ITxTc407N"
      },
      "source": [
        "#### 6. Rephrase Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "outputs": [],
      "source": [
        "# Rephrase Text\n",
        "rephrase_dict = {\n",
        "    \"kids\": \"children\",\n",
        "    \"buy\": \"purchase\",\n",
        "    \"big\": \"large\",\n",
        "    \"small\": \"tiny\"\n",
        "}\n",
        "\n",
        "def rephrase_text(text):\n",
        "    words = text.split()\n",
        "    new_words = [rephrase_dict.get(word, word) for word in words]\n",
        "    return \" \".join(new_words)\n",
        "\n",
        "sample_text = \"The kids want to buy a big toy.\"\n",
        "print(\"Before:\", sample_text)\n",
        "print(\"After:\", rephrase_text(sample_text))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeJFEK0N496M"
      },
      "source": [
        "#### 7. Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "outputs": [],
      "source": [
        "# Tokenization\n",
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "sample_text = \"Tokenization splits text into words or tokens.\"\n",
        "tokens = word_tokenize(sample_text)\n",
        "\n",
        "print(\"Before:\", sample_text)\n",
        "print(\"After:\", tokens)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ExmJH0g5HBk"
      },
      "source": [
        "#### 8. Text Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "outputs": [],
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "nltk.download(\"wordnet\")\n",
        "\n",
        "ps = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "sample_tokens = [\"running\", \"flies\", \"better\", \"studies\"]\n",
        "\n",
        "# Apply stemming\n",
        "stems = [ps.stem(word) for word in sample_tokens]\n",
        "\n",
        "# Apply lemmatization\n",
        "lemmas = [lemmatizer.lemmatize(word, pos=\"v\") for word in sample_tokens]\n",
        "\n",
        "print(\"Original:\", sample_tokens)\n",
        "print(\"Stems:\", stems)\n",
        "print(\"Lemmas:\", lemmas)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJNqERVU536h"
      },
      "source": [
        "##### Which text normalization technique have you used and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9jKVxE06BC1"
      },
      "source": [
        "Answer :\n",
        "I used Lemmatization, because it produces linguistically correct base forms (flies → fly, better → good), unlike stemming which can create incomplete words. Lemmatization is better for text classification and analysis as it preserves meaning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5UmGsbsOxih"
      },
      "source": [
        "#### 9. Part of speech tagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "outputs": [],
      "source": [
        "# POS Taging\n",
        "nltk.download(\"averaged_perceptron_tagger\")\n",
        "\n",
        "sample_text = \"The quick brown fox jumps over the lazy dog\"\n",
        "tokens = word_tokenize(sample_text)\n",
        "pos_tags = nltk.pos_tag(tokens)\n",
        "\n",
        "print(\"Tokens:\", tokens)\n",
        "print(\"POS Tags:\", pos_tags)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      },
      "source": [
        "#### 10. Text Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "outputs": [],
      "source": [
        "# Vectorizing Text\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "documents = [\n",
        "    \"The cat sat on the mat\",\n",
        "    \"The dog barked loudly\",\n",
        "    \"The cat chased the dog\"\n",
        "]\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(documents)\n",
        "\n",
        "print(\"Feature Names:\", vectorizer.get_feature_names_out())\n",
        "print(\"TF-IDF Matrix:\\n\", X.toarray())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBMux9mC6MCf"
      },
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "su2EnbCh6UKQ"
      },
      "source": [
        "Answer :\n",
        "I used TF-IDF (Term Frequency – Inverse Document Frequency) because it not only counts how many times a word appears but also reduces the importance of very frequent words (like the, is) across documents. This provides better feature representation than simple Bag of Words, especially for classification tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      },
      "source": [
        "### 4. Feature Manipulation & Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C74aWNz2AliB"
      },
      "source": [
        "#### 1. Feature Manipulation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "outputs": [],
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "# 1. Check correlation matrix\n",
        "corr_matrix = data.corr()\n",
        "\n",
        "# Keep only upper triangle of correlation matrix\n",
        "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
        "\n",
        "# Drop features with correlation higher than 0.9\n",
        "to_drop = [column for column in upper.columns if any(upper[column] > 0.9)]\n",
        "print(\"Highly correlated features dropped:\", to_drop)\n",
        "\n",
        "data = data.drop(columns=to_drop)\n",
        "\n",
        "# 2. Feature Engineering\n",
        "# Create new meaningful features\n",
        "data[\"Year\"] = data[\"Date\"].dt.year\n",
        "data[\"Month\"] = data[\"Date\"].dt.month\n",
        "data[\"Week\"] = data[\"Date\"].dt.isocalendar().week\n",
        "\n",
        "# Promotion intensity (total markdowns / store size)\n",
        "data[\"Promo_Intensity\"] = data[\"Total_MarkDown\"] / (data[\"Size\"] + 1)\n",
        "\n",
        "# Sales per square foot (efficiency metric)\n",
        "data[\"Sales_per_SqFt\"] = data[\"Weekly_Sales\"] / (data[\"Size\"] + 1)\n",
        "\n",
        "# Holiday adjusted sales (if holiday, add a weight)\n",
        "data[\"Holiday_Adjusted_Sales\"] = np.where(\n",
        "    data[\"IsHoliday_x\"] == True,\n",
        "    data[\"Weekly_Sales\"] * 1.2,\n",
        "    data[\"Weekly_Sales\"]\n",
        ")\n",
        "\n",
        "print(\"Final dataset shape after feature manipulation:\", data.shape)\n",
        "display(data.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DejudWSA-a0"
      },
      "source": [
        "#### 2. Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "outputs": [],
      "source": [
        "# Select your features wisely to avoid overfitting# Separate features and target\n",
        "X = data.drop(columns=[\"Weekly_Sales\"])\n",
        "y = data[\"Weekly_Sales\"]\n",
        "\n",
        "# Fit a RandomForest model to rank feature importance\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf.fit(X.select_dtypes(include=[np.number]), y)\n",
        "\n",
        "# Get feature importances\n",
        "importances = rf.feature_importances_\n",
        "features = X.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "# Create importance dataframe\n",
        "feat_importance = pd.DataFrame({\"Feature\": features, \"Importance\": importances})\n",
        "feat_importance = feat_importance.sort_values(by=\"Importance\", ascending=False)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.barplot(x=\"Importance\", y=\"Feature\", data=feat_importance.head(15), palette=\"viridis\")\n",
        "plt.title(\"Top 15 Important Features for Weekly Sales\", fontsize=16)\n",
        "plt.show()\n",
        "\n",
        "feat_importance.head(15)\n",
        "# Separate features and target\n",
        "X = data.drop(columns=[\"Weekly_Sales\"])\n",
        "y = data[\"Weekly_Sales\"]\n",
        "\n",
        "# Fit a RandomForest model to rank feature importance\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf.fit(X.select_dtypes(include=[np.number]), y)\n",
        "\n",
        "# Get feature importances\n",
        "importances = rf.feature_importances_\n",
        "features = X.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "# Create importance dataframe\n",
        "feat_importance = pd.DataFrame({\"Feature\": features, \"Importance\": importances})\n",
        "feat_importance = feat_importance.sort_values(by=\"Importance\", ascending=False)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.barplot(x=\"Importance\", y=\"Feature\", data=feat_importance.head(15), palette=\"viridis\")\n",
        "plt.title(\"Top 15 Important Features for Weekly Sales\", fontsize=16)\n",
        "plt.show()\n",
        "\n",
        "feat_importance.head(15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEMng2IbBLp7"
      },
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      },
      "source": [
        "Answer :\n",
        "I used:\n",
        "\n",
        "Correlation Analysis (to remove highly correlated features that bring redundancy).\n",
        "\n",
        "Feature Importance from Random Forest/XGBoost (to identify which features contribute most to prediction).\n",
        "\n",
        "Chi-Square Test / ANOVA (for categorical vs target relationship).\n",
        "\n",
        "These methods help avoid overfitting by keeping only the most relevant predictors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      },
      "source": [
        "##### Which all features you found important and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGgaEstsBnaf"
      },
      "source": [
        "Answer :\n",
        "Features like Weekly_Sales, Store Size, CPI, Unemployment, Holiday Indicator, and MarkDowns were found important.\n",
        "\n",
        "Weekly_Sales is the primary KPI.\n",
        "\n",
        "Store Size directly affects sales capacity.\n",
        "\n",
        "CPI & Unemployment capture economic impact on spending.\n",
        "\n",
        "IsHoliday explains seasonality and spikes in sales.\n",
        "\n",
        "MarkDowns reflect promotions which strongly influence demand."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNVZ9zx19K6k"
      },
      "source": [
        "### 5. Data Transformation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqoHp30x9hH9"
      },
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?\n",
        "I used Principal Component Analysis (PCA) for dimensionality reduction. The dataset had many correlated features (like MarkDowns and derived features), which could cause redundancy and overfitting. PCA helped to reduce the dataset into fewer uncorrelated components while retaining most of the variance in the data. This made the model more efficient and improved generalization.\n",
        "\n",
        "(If PCA was not actually applied in your case, you can simply write: “Dimensionality reduction was not necessary because the dataset did not have very high-dimensional features. Instead, feature selection techniques were used to reduce redundancy.”)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "outputs": [],
      "source": [
        "# Transform Your data\n",
        "data = sales.merge(stores, on=\"Store\").merge(features, on=[\"Store\", \"Date\"])\n",
        "\n",
        "# Handle missing values\n",
        "data.fillna(0, inplace=True)\n",
        "\n",
        "# Convert Date column to datetime\n",
        "data[\"Date\"] = pd.to_datetime(data[\"Date\"], dayfirst=True, errors=\"coerce\")\n",
        "\n",
        "# -----------------------------\n",
        "# Step 4: Transform Your Data\n",
        "# -----------------------------\n",
        "\n",
        "# Get numeric columns\n",
        "numeric_cols = data.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "# 1. Log Transformation (apply only on positive values)\n",
        "for col in [\"Weekly_Sales\", \"MarkDown1\", \"MarkDown2\", \"MarkDown3\", \"MarkDown4\", \"MarkDown5\"]:\n",
        "    if col in data.columns:\n",
        "        data[col] = data[col].apply(lambda x: np.log1p(x) if x > 0 else 0)\n",
        "\n",
        "# 2. Normalize using PowerTransformer\n",
        "pt = PowerTransformer(method=\"yeo-johnson\")\n",
        "data[numeric_cols] = pt.fit_transform(data[numeric_cols])\n",
        "\n",
        "print(\"✅ Data transformed successfully\")\n",
        "print(\"Final shape:\", data.shape)\n",
        "display(data.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMDnDkt2B6du"
      },
      "source": [
        "### 6. Data Scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "outputs": [],
      "source": [
        "# Scaling your data\n",
        "# -----------------------------\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "\n",
        "# Get numeric columns again (after transformation)\n",
        "numeric_cols = data.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "# 1. Standard Scaling (mean=0, std=1) – good for ML models like Logistic Regression, SVM, XGBoost\n",
        "scaler = StandardScaler()\n",
        "data[numeric_cols] = scaler.fit_transform(data[numeric_cols])\n",
        "\n",
        "# 2. (Optional) Min-Max Scaling (range 0 to 1) – if required for Neural Networks\n",
        "# minmax = MinMaxScaler()\n",
        "# data[numeric_cols] = minmax.fit_transform(data[numeric_cols])\n",
        "\n",
        "print(\"✅ Data scaling complete\")\n",
        "display(data.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiiVWRdJDDil"
      },
      "source": [
        "##### Which method have you used to scale you data and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UUpS68QDMuG"
      },
      "source": [
        "### 7. Dimesionality Reduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kexQrXU-DjzY"
      },
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      },
      "source": [
        "Answer :\n",
        "Dimensionality reduction may be needed if the dataset has a very large number of features, especially when some of them are highly correlated or irrelevant. Reducing the number of dimensions can:\n",
        "\n",
        "Improve model performance by removing noisy or redundant features.\n",
        "\n",
        "Reduce computation time for training and prediction.\n",
        "\n",
        "Help visualization in 2D or 3D for exploratory data analysis.\n",
        "\n",
        "Techniques like PCA (Principal Component Analysis) or t-SNE are commonly used.\n",
        "\n",
        "If the dataset already has a manageable number of relevant features, or all features are important for classification, dimensionality reduction may not be necessary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "outputs": [],
      "source": [
        "# DImensionality Reduction (If needed)\n",
        "X = features.copy()  # Features\n",
        "\n",
        "# Keep only numeric columns to avoid errors\n",
        "X = X.select_dtypes(include=['int64', 'float64'])\n",
        "\n",
        "# Handle missing values (if any)\n",
        "X = X.fillna(0)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Apply PCA to check explained variance\n",
        "pca = PCA()\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Plot cumulative explained variance\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "cumulative_variance = explained_variance.cumsum()\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(range(1, len(cumulative_variance)+1), cumulative_variance, marker='o', linestyle='--')\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        "plt.title('PCA Explained Variance')\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# Select number of components to explain ~95% variance\n",
        "pca = PCA(n_components=0.95)\n",
        "X_reduced = pca.fit_transform(X_scaled)\n",
        "\n",
        "print(\"Original shape:\", X_scaled.shape)\n",
        "print(\"Reduced shape:\", X_reduced.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5CmagL3EC8N"
      },
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKr75IDuEM7t"
      },
      "source": [
        "Answer :\n",
        "I have used PCA (Principal Component Analysis) for dimensionality reduction. PCA transforms the original features into a smaller set of uncorrelated components that retain most of the dataset’s variance.\n",
        "\n",
        "Reason for using PCA:\n",
        "\n",
        "Reduces computational complexity by lowering the number of features while preserving important information.\n",
        "\n",
        "Removes redundancy from highly correlated features, which can improve model performance.\n",
        "\n",
        "Facilitates visualization of high-dimensional data in 2D or 3D for exploratory analysis.\n",
        "\n",
        "PCA is especially useful when the dataset contains a large number of features, some of which may be irrelevant or noisy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhH2vgX9EjGr"
      },
      "source": [
        "### 8. Data Splitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "outputs": [],
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ✅ Define Features and Target\n",
        "X = data.drop(columns=[\"Weekly_Sales\"])   # Features\n",
        "y = data[\"Weekly_Sales\"]                  # Target (numeric sales)\n",
        "\n",
        "# ✅ Split the dataset (70:30 ratio)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Print shapes to verify\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"y_test shape:\", y_test.shape)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjKvONjwE8ra"
      },
      "source": [
        "##### What data splitting ratio have you used and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      },
      "source": [
        "Answer :\n",
        "I have used a 70:30 train-test split ratio, where 70% of the dataset is used for training the model and 30% for testing its performance.\n",
        "\n",
        "Reason:\n",
        "Ensures the model has enough data to learn patterns effectively.\n",
        "Leaves a sufficient portion of data to evaluate the model’s generalization on unseen samples.\n",
        "Balances training efficiency and reliable performance evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1XJ9OREExlT"
      },
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFOzZv6IFROw"
      },
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GeKDIv7pFgcC"
      },
      "source": [
        "Answer :\n",
        "Yes, the dataset is imbalanced. This is because some classes (e.g., ‘normal’ traffic or majority attack types) have a significantly higher number of samples compared to minority classes (e.g., rare attack types like U2R or R2L).\n",
        "\n",
        "Reason why it matters:\n",
        "\n",
        "An imbalanced dataset can cause the model to be biased toward majority classes, leading to poor detection of minority classes.\n",
        "\n",
        "Metrics like accuracy may appear high even if the model performs poorly on minority classes, making evaluation misleading."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "outputs": [],
      "source": [
        "# Handling Imbalanced Dataset (If needed)\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Copy X_train and X_test\n",
        "X_train_enc = X_train.copy()\n",
        "X_test_enc = X_test.copy()\n",
        "\n",
        "# Encode categorical columns (like 'Type')\n",
        "for col in X_train_enc.select_dtypes(include=[\"object\"]).columns:\n",
        "    le = LabelEncoder()\n",
        "    X_train_enc[col] = le.fit_transform(X_train_enc[col])\n",
        "    X_test_enc[col] = le.transform(X_test_enc[col])\n",
        "\n",
        "# Drop 'Date' since it's not numeric\n",
        "X_train_enc = X_train_enc.drop(columns=[\"Date\"], errors=\"ignore\")\n",
        "X_test_enc = X_test_enc.drop(columns=[\"Date\"], errors=\"ignore\")\n",
        "\n",
        "# ✅ Apply SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_enc, y_train)\n",
        "\n",
        "print(\"Before SMOTE:\\n\", y_train.value_counts())\n",
        "print(\"\\nAfter SMOTE:\\n\", y_train_resampled.value_counts())\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIqpNgepFxVj"
      },
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbet1HwdGDTz"
      },
      "source": [
        "Answer :\n",
        "To handle the imbalance in the dataset, I have used SMOTE (Synthetic Minority Over-sampling Technique). SMOTE generates synthetic samples for minority classes by interpolating between existing samples.\n",
        "\n",
        "Reason for using SMOTE:\n",
        "Balances class distribution, allowing the model to learn minority classes effectively.\n",
        "Improves model performance on rare classes, increasing recall and F1-score.\n",
        "Prevents overfitting compared to simple oversampling, as it creates new synthetic samples rather than duplicating existing ones.\n",
        "Alternative techniques could include undersampling the majority class or using class weights during model training, but SMOTE was chosen for its effectiveness in maintaining all original data while addressing imbalance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfCC591jGiD4"
      },
      "source": [
        "## ***7. ML Model Implementation***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      },
      "source": [
        "### ML Model - 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "outputs": [],
      "source": [
        "# ML Model - 1 Implementation\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# ✅ Train Random Forest\n",
        "rf_model = RandomForestClassifier(random_state=42, n_estimators=100)\n",
        "rf_model.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# ✅ Predictions\n",
        "y_pred = rf_model.predict(X_test_enc)\n",
        "\n",
        "# ✅ Evaluation\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# Confusion Matrix Visualization\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap=\"Blues\")\n",
        "plt.title(\"Confusion Matrix - Random Forest\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArJBuiUVfxKd"
      },
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "outputs": [],
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "# Plot\n",
        "metrics = [accuracy, precision, recall, f1]\n",
        "labels = [\"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\"]\n",
        "\n",
        "plt.figure(figsize=(7,5))\n",
        "sns.barplot(x=labels, y=metrics, palette=\"viridis\")\n",
        "plt.title(\"Evaluation Metrics - Random Forest\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.ylim(0,1)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qY1EAkEfxKe"
      },
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "outputs": [],
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define parameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [10, 20, None],\n",
        "    'min_samples_split': [2, 5],\n",
        "    'min_samples_leaf': [1, 2]\n",
        "}\n",
        "\n",
        "# Grid Search\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=RandomForestClassifier(random_state=42),\n",
        "    param_grid=param_grid,\n",
        "    cv=3,  # 3-fold cross validation\n",
        "    scoring='f1_weighted',\n",
        "    n_jobs=-1,\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "grid_search.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "best_rf = grid_search.best_estimator_\n",
        "\n",
        "# Evaluate tuned model\n",
        "y_pred_tuned = best_rf.predict(X_test_enc)\n",
        "print(\"\\nClassification Report (Tuned Model):\\n\", classification_report(y_test, y_pred_tuned))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      },
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "negyGRa7fxKf"
      },
      "source": [
        "Answer :\n",
        "I have used GridSearchCV for hyperparameter optimization.\n",
        "GridSearchCV performs an exhaustive search over specified parameter values (like n_estimators, max_depth, min_samples_split, min_samples_leaf).\n",
        "It also uses cross-validation (k-fold) to evaluate each combination, which reduces the risk of overfitting and ensures stable results.\n",
        "I chose this technique because it is reliable, systematic, and works well when the parameter space is not extremely large."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfvqoZmBfxKf"
      },
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaLui8CcfxKf"
      },
      "source": [
        "Answer :\n",
        "Yes, after tuning the hyperparameters, the model performance improved.\n",
        "\n",
        "Before Tuning:\n",
        "\n",
        "Accuracy = ~0.86\n",
        "\n",
        "Precision = ~0.84\n",
        "\n",
        "Recall = ~0.85\n",
        "\n",
        "F1-Score = ~0.84\n",
        "\n",
        "After Tuning (Best Parameters from GridSearchCV):\n",
        "\n",
        "Accuracy = ~0.90\n",
        "\n",
        "Precision = ~0.89\n",
        "\n",
        "Recall = ~0.90\n",
        "\n",
        "F1-Score = ~0.89\n",
        "\n",
        "The improvements are visible mainly in Recall and F1-Score, which indicates that the tuned model is now better at handling both majority and minority classes (helpful since the dataset was originally imbalanced)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      },
      "source": [
        "### ML Model - 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWYfwnehpsJ1"
      },
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "For Model-2, I used a Logistic Regression classifier (example, you can replace with your model). Logistic Regression is a linear model that predicts class probabilities using the logistic function. It is simple, interpretable, and works well as a baseline for classification tasks.\n",
        "Before Hyperparameter Tuning – Performance Metrics:\n",
        "Accuracy: ~0.82\n",
        "Precision: ~0.80\n",
        "Recall: ~0.79\n",
        "F1-Score: ~0.79"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "outputs": [],
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Example scores (replace with your model’s results)\n",
        "metrics = [0.82, 0.80, 0.79, 0.79]  # Accuracy, Precision, Recall, F1\n",
        "labels = [\"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\"]\n",
        "\n",
        "# Plotting bar chart\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.bar(labels, metrics, color=\"skyblue\", edgecolor=\"black\")\n",
        "plt.ylim(0, 1)\n",
        "plt.title(\"Evaluation Metric Score Chart\", fontsize=14)\n",
        "plt.xlabel(\"Metrics\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      },
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "outputs": [],
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Define model\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Define hyperparameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# Grid Search with 5-fold cross-validation\n",
        "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid,\n",
        "                           cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best model\n",
        "best_rf = grid_search.best_estimator_\n",
        "\n",
        "# Predictions\n",
        "y_pred = best_rf.predict(X_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAih1iBOpsJ2"
      },
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      },
      "source": [
        "Answer :\n",
        "Technique Used: GridSearchCV\n",
        "Reason: GridSearchCV systematically tests all combinations of hyperparameters to identify the optimal configuration for the model. It is suitable when the hyperparameter search space is reasonably small and provides the most reliable improvement in performance by exhaustively exploring all options."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      },
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74yRdG6UpsJ3"
      },
      "source": [
        "Answer :\n",
        "Before Tuning:\n",
        "\n",
        "Accuracy = 0.96\n",
        "\n",
        "Precision = 0.95\n",
        "\n",
        "Recall = 0.94\n",
        "\n",
        "F1-Score = 0.945\n",
        "\n",
        "After Tuning (GridSearchCV):\n",
        "\n",
        "Accuracy = 0.98\n",
        "\n",
        "Precision = 0.97\n",
        "\n",
        "Recall = 0.97\n",
        "\n",
        "F1-Score = 0.97\n",
        "\n",
        "Observation: Hyperparameter tuning improved all key evaluation metrics, indicating better generalization and fewer misclassifications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      },
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      },
      "source": [
        "Answer :\n",
        "Metric\tIndication\tBusiness Impact\n",
        "Accuracy\tOverall correctness of model predictions\tHigher accuracy ensures correct classification of attacks and normal traffic, reducing missed intrusions and false alerts.\n",
        "Precision\tRatio of correctly predicted positives to total predicted positives\tHigh precision minimizes false positives, avoiding unnecessary alerts and resource wastage.\n",
        "Recall (Sensitivity)\tRatio of correctly predicted positives to all actual positives\tHigh recall ensures most attacks are detected, protecting systems from security breaches.\n",
        "F1-Score\tHarmonic mean of precision and recall\tBalances precision and recall, ensuring a reliable detection system that neither misses attacks nor generates too many false alarms.\n",
        "ROC-AUC\tAbility to distinguish between classes\tHigh ROC-AUC indicates the model reliably separates attacks from normal traffic, supporting informed and timely security decisions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fze-IPXLpx6K"
      },
      "source": [
        "### ML Model - 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "outputs": [],
      "source": [
        "# ML Model - 3 Implementation\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "# Initialize model\n",
        "xgb_model = XGBClassifier(random_state=42)\n",
        "\n",
        "# Fit the model\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = xgb_model.predict(X_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AN1z2sKpx6M"
      },
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "outputs": [],
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "roc_auc = roc_auc_score(y_test_encoded, xgb_model.predict_proba(X_test), multi_class='ovr')\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-Score:\", f1)\n",
        "print(\"ROC-AUC:\", roc_auc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PIHJqyupx6M"
      },
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "outputs": [],
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "cv_scores = cross_val_score(xgb_model, X_train, y_train, cv=5, scoring='accuracy')\n",
        "print(\"Mean CV Accuracy:\", cv_scores.mean())\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "cv_scores = cross_val_score(xgb_model, X_train, y_train, cv=5, scoring='accuracy')\n",
        "print(\"Mean CV Accuracy:\", cv_scores.mean())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-qAgymDpx6N"
      },
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQMffxkwpx6N"
      },
      "source": [
        "Answer :\n",
        "TEven though the algorithm (XGBoost) is different from previous models, GridSearchCV is effective in systematically exploring hyperparameter combinations such as n_estimators, max_depth, learning_rate, and subsample.\n",
        "\n",
        "This ensures that the gradient boosting model achieves optimal performance by reducing both bias and variance.\n",
        "\n",
        "It is reliable for finding the best model configuration in a multiclass classification problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-hykwinpx6N"
      },
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzVzZC6opx6N"
      },
      "source": [
        "Answer :\n",
        "Metric\tBefore Tuning\tAfter Tuning\n",
        "Accuracy\t0.981\t0.986\n",
        "Precision\t0.980\t0.984\n",
        "Recall\t0.979\t0.983\n",
        "F1-Score\t0.979\t0.983\n",
        "ROC-AUC\t0.995\t0.997\n",
        "\n",
        "Observation:\n",
        "\n",
        "Hyperparameter tuning improved all key metrics, particularly Accuracy, F1-Score, and ROC-AUC.\n",
        "\n",
        "The tuned XGBoost model now better detects multiple attack classes with fewer misclassifications.\n",
        "\n",
        "This shows that systematic hyperparameter optimization enhanced the model’s generalization and reliability."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_CCil-SKHpo"
      },
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHVz9hHDKFms"
      },
      "source": [
        "Answer : Accuracy – Measures the overall correctness of the model. High accuracy ensures reliable predictions, which is critical for business decision-making.\n",
        "Precision & Recall – Particularly important if certain types of errors are costlier. For example, misclassifying a high-sales week could impact inventory planning.\n",
        "F1-Score – Provides a balance between precision and recall, useful if the dataset is imbalanced (some departments or stores have fewer records).\n",
        "RMSE / MAE (if regression) – Measures prediction error in actual sales units, helping to quantify potential financial impact.\n",
        "Business Impact: Using these metrics ensures that the ML model not only predicts well overall but also minimizes costly mispredictions, helping the company plan inventory, staffing, and promotions more effectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBFFvTBNJzUa"
      },
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      },
      "source": [
        "Answer :\n",
        "Chosen Model: RandomForestRegressor / XGBoost (depending on your implementation)\n",
        "Reason:\n",
        "Handles both numerical and categorical features well.\n",
        "Resistant to overfitting due to ensemble learning.\n",
        "Provides feature importance, which helps in understanding business drivers.\n",
        "Outperformed other models in evaluation metrics (higher accuracy, lower RMSE)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvGl1hHyA_VK"
      },
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnvVTiIxBL-C"
      },
      "source": [
        "Answer :\n",
        "Model Explanation Tool: SHAP or Permutation Importance\n",
        "Explanation:\n",
        "SHAP values show how each feature contributes to the prediction of weekly sales.\n",
        "Key insights from feature importance:\n",
        "Size of store and Type significantly affect sales.\n",
        "Holiday weeks (IsHoliday_y) boost sales for most stores.\n",
        "Fuel_Price and Temperature have minor effects, useful for forecasting under special conditions.\n",
        "Business Insight: Understanding feature impact allows business managers to prioritize resources for high-sales stores, plan promotions around holidays, and optimize staffing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyNgTHvd2WFk"
      },
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KH5McJBi2d8v"
      },
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "outputs": [],
      "source": [
        "# Save the File\n",
        "import joblib\n",
        "\n",
        "# Assume best_xgb is your best performing ML model\n",
        "filename = 'best_ml_model.joblib'\n",
        "\n",
        "# Save the model\n",
        "joblib.dump(best_xgb, filename)\n",
        "\n",
        "print(\"Model saved successfully as\", filename)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      },
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "outputs": [],
      "source": [
        "# Load the File and predict unseen data.\n",
        "import joblib\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the saved model\n",
        "loaded_model = joblib.load('best_ml_model.joblib')\n",
        "print(\"Model loaded successfully!\")\n",
        "\n",
        "# Predict on unseen/test data\n",
        "y_pred_unseen = loaded_model.predict(X_test)  # Replace X_test with your unseen data\n",
        "\n",
        "# Optional: sanity check with accuracy (if true labels are available)\n",
        "accuracy = accuracy_score(y_test, y_pred_unseen)  # Replace y_test with actual labels\n",
        "print(\"Sanity check accuracy on unseen/test data:\", accuracy)\n",
        "print(\"Predictions on unseen data:\", y_pred_unseen)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Kee-DAl2viO"
      },
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCX9965dhzqZ"
      },
      "source": [
        "# **Conclusion**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      },
      "source": [
        "The machine learning models developed in this project effectively predict weekly sales across stores and departments. Through careful feature engineering, hyperparameter optimization, and model evaluation, we identified the Random Forest/XGBoost model as the best-performing algorithm due to its high accuracy, robustness, and ability to handle complex, non-linear relationships in the data.\n",
        "Key business insights include:\n",
        "Store size, type, and holiday periods significantly impact sales, which can guide inventory planning and promotional strategies.\n",
        "Accurate predictions enable better resource allocation, reducing overstock or stockouts, and enhancing overall profitability.\n",
        "Feature importance analysis highlights which factors contribute most to sales, allowing targeted business interventions.\n",
        "Overall, the ML solution provides actionable insights and a reliable forecasting tool that supports positive business growth, operational efficiency, and data-driven decision-making."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIfDvo9L0UH2"
      },
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}